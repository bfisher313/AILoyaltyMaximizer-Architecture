# 4. Data Architecture

The Data Architecture for the AI Loyalty Maximizer Suite is fundamental to its ability to provide intelligent and accurate insights. This section describes how data is structured, stored, processed, and managed across the system, with a particular focus on the knowledge graph that powers its core reasoning capabilities and the pipeline designed to populate it.

## 4.1. Overview (Data at Rest, Data in Motion)

The system handles several categories of data:

* **User Data:** Information related to users, their loyalty program memberships, preferences, and potentially saved query results or itineraries. This data requires appropriate security and privacy considerations.
* **Loyalty Program Knowledge Data:** A complex and dynamic dataset comprising airline loyalty program rules, earning charts, redemption policies, airline partnerships, fare class details, airport information, and other related domain knowledge. This forms the core of the system's "intelligence."
* **Raw Source Data:** The initially gathered web pages, PDFs, and text files containing loyalty program information, which serve as the input to the data ingestion pipeline.
* **Intermediate Processed Data:** Data at various stages of transformation within the ingestion pipeline (e.g., Textract output, LLM-extracted JSON, data prepared for graph loading).
* **Application & Operational Data:** Logs, metrics, and other operational data generated by the system components.

**Data at Rest:**
All persistent data, including the user profiles in Amazon DynamoDB, the loyalty program knowledge graph in Amazon Neptune, raw and processed files in Amazon S3, and application backups, will be encrypted at rest using AWS-managed encryption services (e.g., KMS, S3-SSE, DynamoDB default encryption, Neptune default encryption) as detailed in the Security Architecture (Section 6.1).

**Data in Motion:**
All data transmitted between system components, between users and the system, and between internal AWS services will be encrypted in transit using HTTPS/TLS. This ensures data confidentiality and integrity during communication.

## 4.2. Conceptual Data Model (Key Business Entities)

Before diving into specific database schemas, it's useful to define the core conceptual entities that the AI Loyalty Maximizer Suite deals with. These entities and their relationships form the basis of our knowledge graph and other data stores.

**[ðŸš§ TODO: Create Conceptual Data Model (ERD-like). See GitHub Issue #4 ðŸš§]**

* **User:** Represents an end-user of the application.
    * *Attributes (examples):* UserID, Preferences (home airport, preferred airlines/alliances), Stored Loyalty Program Memberships.
* **Loyalty Program:** A specific frequent flyer or loyalty program offered by an airline or alliance.
    * *Attributes (examples):* ProgramID, ProgramName, OwningAirline, Alliance, RedeemableCurrencyName, EliteTierStructure.
* **Airline:** An airline carrier.
    * *Attributes (examples):* AirlineCode (IATA/ICAO), AirlineName, AllianceMembership.
* **Flight Segment:** A single leg of a flight.
    * *Attributes (examples):* MarketingCarrier, OperatingCarrier, FlightNumber, OriginAirport, DestinationAirport, FareClass, Distance.
* **Fare Class:** A specific booking code or fare bucket on a flight.
    * *Attributes (examples):* FareCode, CabinType (Economy, Business, First), EarningPotentialDetails.
* **Earning Rule:** Defines how miles/points or status credits are earned for a given flight, fare class, and loyalty program.
    * *Attributes (examples):* AppliesToFareClass, AppliesToAirline, CreditingProgram, RedeemableRate (e.g., % of distance, per dollar), EliteMetricRate, Conditions, EffectiveDate, ExpirationDate.
* **Redemption Rule/Award Chart Entry:** Defines the cost to redeem rewards for a specific award (e.g., flight, upgrade).
    * *Attributes (examples):* ProgramID, OriginRegion, DestinationRegion, CabinClass, PointsCost, CoPay, ApplicableAirlines, Conditions.
* **Airport:** An airport.
    * *Attributes (examples):* AirportCode (IATA), AirportName, City, Country, Region.
* **Alliance:** An airline alliance (e.g., Star Alliance, Oneworld, SkyTeam).
    * *Attributes (examples):* AllianceName, MemberAirlines.
* **Transfer Partnership:** A relationship allowing transfer of rewards currency between programs.
    * *Attributes (examples):* FromProgram, ToProgram, TransferRatio, Conditions.

*(This is a high-level conceptual model. Detailed attributes and relationships will be refined in the GraphRAG schema and other data store designs.)*

## 4.3. GraphRAG Schema & Knowledge Base (Amazon Neptune)

The heart of the AI Loyalty Maximizer Suite's intelligence lies in its knowledge graph, which will be implemented using **Amazon Neptune**. This graph database will store the intricate relationships between airlines, loyalty programs, earning rules, redemption options, partnerships, and other domain-specific entities. The **Retrieval Augmented Generation (RAG)** technique will leverage this graph to provide rich, accurate context to the LLM for answering user queries.

### 4.3.1. Rationale for Graph Database (Neptune)
* **Complex Relationships:** Loyalty program data is characterized by many-to-many relationships, hierarchical structures (alliances, programs, tiers), and indirect connections (e.g., earning on a partner airline credited to another program). Graph databases excel at modeling and querying such interconnected data.
* **Flexible Schema:** Loyalty rules change frequently. While a core schema will be defined, graph databases offer flexibility to evolve the model as new types of rules or partnerships emerge.
* **Efficient Traversal Queries:** Finding optimal award paths, calculating earnings through multiple partner layers, or identifying "sweet spot" redemptions often involves traversing complex relationships, which is highly efficient in a graph database.
* **AWS Managed Service:** Neptune is a fully managed graph database service, reducing operational overhead for provisioning, patching, backups, and scaling. It integrates well with other AWS services used in the architecture.
* **Support for Query Languages:** Neptune supports popular graph query languages like Apache TinkerPop Gremlin and openCypher, providing powerful ways to interact with the graph.

### 4.3.2. Conceptual Graph Schema (Nodes & Edges)

The Amazon Neptune graph database will store entities and their relationships as nodes and edges, respectively. Below is a conceptual schema outlining key node labels, their representative properties, and key edge labels with their significance. This schema will be refined and expanded as the system evolves.

**[ðŸš§ TODO: Insert Graph Schema Diagram (Neptune) here. See GitHub Issue #5 ðŸš§]**

**Key Node Labels & Representative Properties:**

* **`Airline`**
    * `airlineId` (String, Primary Key): Unique identifier, often IATA or ICAO code (e.g., "UA", "AAL").
    * `name` (String): Official name of the airline (e.g., "United Airlines", "American Airlines").
    * `iataCode` (String): Two-letter IATA airline designator.
    * `icaoCode` (String): Three-letter ICAO airline designator.
    * `allianceName` (String, Optional): Name of the primary alliance the airline belongs to (e.g., "Star Alliance", "Oneworld", "SkyTeam").
    * `countryOfOrigin` (String): The airline's primary country of operation.
    * `website` (String, Optional): URL to the airline's official website.

* **`LoyaltyProgram`**
    * `programId` (String, Primary Key): Unique identifier for the loyalty program (e.g., "UA_MileagePlus", "AA_AAdvantage", "DL_SkyMiles").
    * `name` (String): Official name of the loyalty program (e.g., "MileagePlus", "AAdvantage", "SkyMiles").
    * `governingAirlineId` (String, Foreign Key to Airline): The primary airline that owns/governs the program.
    * `redeemableCurrencyName` (String): Name of the program's rewards currency (e.g., "Miles", "Points", "Avios").
    * `eliteTierSystemDescription` (String, Optional): Brief overview of how elite status is structured.
    * `website` (String, Optional): URL to the loyalty program's official website.

* **`Airport`**
    * `airportId` (String, Primary Key): Unique identifier, typically IATA code (e.g., "JFK", "LAX", "LHR").
    * `name` (String): Official name of the airport (e.g., "John F. Kennedy International Airport").
    * `city` (String): Primary city served by the airport.
    * `country` (String): Country where the airport is located.
    * `iataCode` (String): Three-letter IATA airport code.
    * `icaoCode` (String, Optional): Four-letter ICAO airport code.
    * **`latitude` (Float): Geographic latitude of the airport (e.g., 40.6413).**
    * **`longitude` (Float): Geographic longitude of the airport (e.g., -73.7781).**
    * `region` (String, Optional): Broader geographic region for easier regional rule application (e.g., "North America", "Europe", "Southeast Asia").


* **`FareClass`** (Represents a booking code within a cabin)
    * `fareClassCode` (String): The specific booking code (e.g., "Y", "J", "K", "P").
    * `cabinType` (String): The associated cabin (e.g., "Economy", "Premium Economy", "Business", "First").
    * `description` (String, Optional): Brief description or common name (e.g., "Full Fare Economy", "Discounted Business").
    * `airlineContext` (String, Optional): The airline for which this fare class definition is most relevant, as codes can vary.

* **`EarningRule`** (Represents a specific rule for how rewards are accrued)
    * `ruleId` (String, Primary Key): Unique identifier for this specific earning rule instance.
    * `description` (String): A human-readable summary of the rule (e.g., "Earn 100% redeemable miles and 1 PQP per dollar on UA operated flights in H class when credited to MileagePlus").
    * `redeemableMilesFormula` (String): Formula or value for calculating redeemable miles (e.g., "distance * 1.0", "fare * 5", "500").
    * `eliteMetricName` (String, Optional): Name of the primary elite qualifying metric (e.g., "PQP", "LP", "Tier Miles").
    * `eliteMetricFormula` (String, Optional): Formula or value for calculating the elite metric.
    * `additionalEliteMetrics` (List of Objects, Optional): For programs with multiple EQMs (e.g., `[{name: "EQS", value: "1"}]`).
    * `conditions` (String/Text): Specific conditions or restrictions for this rule (e.g., "Ticketed by X", "Flown by Y", "Only for flights within Z region").
    * `effectiveDate` (Date, ISO 8601 String): Date when the rule becomes effective.
    * `expirationDate` (Date, ISO 8601 String, Optional): Date when the rule expires.
    * `sourceDocumentId` (String, Foreign Key to WebSourceDocument): Reference to the source of this rule.

* **`WebSourceDocument`** (Represents the origin of the ingested information)
    * `documentId` (String, Primary Key): Unique identifier for the source document (e.g., hash of URL + timestamp, or S3 URI).
    * `sourceUrl` (String, Optional): The original URL if the source was a web page.
    * `documentType` (String): e.g., "HTML_Page", "PDF_Chart", "Text_Snippet".
    * `lastProcessedTimestamp` (Timestamp): When this document was last processed by the ingestion pipeline.
    * `dataCuratorNotes` (String, Optional): Notes from the Data Curator about this source.

**Key Edge Labels & Representative Properties:**

* **`HAS_LOYALTY_PROGRAM`**
    * Connects: `Airline` -> `LoyaltyProgram`
    * Description: Indicates an airline owns or is primarily associated with a loyalty program.

* **`APPLIES_TO_FLIGHT_CHARACTERISTICS`**
    * Connects: `EarningRule` -> `Airline` (Operating Carrier)
    * Connects: `EarningRule` -> `Airline` (Marketing Carrier, if relevant to rule)
    * Connects: `EarningRule` -> `FareClass`
    * Connects: `EarningRule` -> `Airport` (Origin/Destination if rule is route/region specific)
    * Description: Links an earning rule to the specific flight attributes it applies to.
    * Properties (on edge, if needed): `isOperatingCarrier` (Boolean), `isMarketingCarrier` (Boolean).

* **`CREDITS_TO_PROGRAM`**
    * Connects: `EarningRule` -> `LoyaltyProgram`
    * Description: Specifies which loyalty program an earning rule accrues benefits for.

* **`PARTNERS_WITH`**
    * Connects: `Airline` -> `Airline` (Bidirectional or two directed edges)
    * Connects: `LoyaltyProgram` -> `LoyaltyProgram` (Bidirectional or two directed edges)
    * Description: Represents partnership agreements (e.g., alliance membership, codeshare, interline earning, points transfer).
    * Properties: `partnershipType` (String: "Alliance", "Codeshare", "EarnAlliance", "RedeemAlliance", "PointsTransfer"), `details` (String, e.g., "Star Alliance Member", "Transfer ratio 1:0.8").

* **`EXTRACTED_FROM`**
    * Connects: `EarningRule` -> `WebSourceDocument` (or other extracted entities like `FareClass` specific details)
    * Description: Links the structured data back to its original source document for traceability and verification.

* **`HAS_ELITE_TIER`**
    * Connects: `LoyaltyProgram` -> `EliteStatusTier` (where `EliteStatusTier` could be another node type)
    * Description: Defines the different status levels within a program.
    * Properties on `EliteStatusTier` node: `tierName`, `qualificationCriteriaText`, `keyBenefits`.

**How RAG Leverages This Graph (Conceptual Examples):**

* **Query:** "How many United miles do I earn on a Lufthansa flight from Frankfurt to NYC in K class?"
    * **RAG Process:**
        1.  LLM identifies entities: `United (MileagePlus)`, `Lufthansa`, `Frankfurt (FRA)`, `NYC (JFK/EWR)`, `K class`.
        2.  Graph query retrieves:
            * `Lufthansa` node and its `PARTNERS_WITH` edges to find its relationship with `United MileagePlus` (e.g., via Star Alliance).
            * Relevant `EarningRule` nodes linked to `Lufthansa` (as operating carrier), `K class`, and `United MileagePlus` (as crediting program).
            * These rules would contain formulas and conditions.
        3.  The retrieved rules, partnership details, and formulas are passed as context to the LLM.
        4.  LLM synthesizes the answer, performing calculations if needed.

* **Query:** "Best way to use 100k Amex points for business class to Paris?"
    * **RAG Process:**
        1.  LLM identifies: `Amex (Membership Rewards)`, `Business Class`, `Paris (CDG/ORY)`.
        2.  Graph query retrieves:
            * `Amex Membership Rewards` (as a `LoyaltyProgram` node, or a representation of a transferable currency).
            * Its `HAS_TRANSFER_PARTNER` edges to various airline `LoyaltyProgram` nodes, along with transfer ratios.
            * For each relevant airline program, conceptual `AwardChartEntry` data or `RedemptionRule` patterns for `Business Class` to `Paris`.
        3.  The retrieved transfer options and potential redemption costs/rules are passed to the LLM.
        4.  LLM analyzes and synthesizes the best options.

## 4.4. Other Data Stores

While the Neptune graph database is central for complex loyalty rules, other data stores will be used for specific purposes:

* **Amazon DynamoDB (for User Profiles):**
    * **Purpose:** To store user-specific information such as registered loyalty program memberships, current elite statuses, user preferences (e.g., home airport, preferred airlines/alliances), and potentially saved query results or itineraries.
    * **Rationale:** DynamoDB offers high scalability, low-latency access, and a flexible schema suitable for user profile data. It aligns well with the serverless nature of other components like AWS Lambda.
    * **Conceptual Structure:** Keyed by `UserID`, with attributes for loyalty program details (list of objects), preferences (map), etc. Global Secondary Indexes (GSIs) may be used for querying by attributes other than `UserID` if needed (e.g., finding users with a specific preference, though this is less critical for core functionality).

* **Amazon S3 (for various storage needs):**
    * **Raw Source Data:** `s3://loyalty-rules-raw-pages/` - Landing zone for manually gathered HTML, PDF, and text files containing loyalty program information.
    * **Processed Text/Intermediate Data:** `s3://loyalty-rules-processed-text/` - Stores cleaned text or structured output from Textract/initial Lambda processing before full ETL.
    * **LLM Extraction Output:** `s3://loyalty-rules-llm-extracted-facts/` - Stores structured JSON output from the LLM-based information extraction step in the pipeline. This can be used for Athena-based validation.
    * **Neptune Load Files:** `s3://loyalty-rules-neptune-load-files/` - Stores CSV files formatted for Neptune's bulk loader, generated by the Glue ETL jobs.
    * **RAG Supplementary Documents:** `s3://loyalty-rag-supplementary-docs/` - If detailed terms & conditions documents or other large text sources are used to supplement the graph for RAG.
    * **Application Logs & Backups:** Standard S3 usage for storing access logs, application logs, and database backups.
    * **Rationale:** S3 provides durable, scalable, and cost-effective object storage suitable for a wide variety of data types and pipeline staging areas.

## 4.5. Initial Data Ingestion & Curation Strategy

While the long-term vision involves the "Automated Knowledge Base Ingestion Pipeline" (detailed in Section 4.6), the initial population and ongoing curation of the knowledge base will rely on:

* **Manual Identification & Gathering of Source Material:** The `Data Curator` persona will be responsible for identifying authoritative sources (official airline websites, loyalty program terms & conditions pages, partner airline pages) and manually saving relevant web pages or documents.
* **Prioritization:** Focus will initially be on a core set of major airlines and their loyalty programs/alliances to build a foundational knowledge base.
* **Version Control for Source Links (Conceptual):** A simple mechanism (e.g., a shared document or a simple database table) might be used initially to track the URLs and last-checked dates of source pages to aid the Data Curator, though this is less a part of the core automated pipeline itself.
* **Feeding the Automated Pipeline:** The manually gathered files will be uploaded to the designated S3 bucket to trigger the automated processing and ingestion pipeline.

This strategy acknowledges the complexity of fully automated web scraping and instead focuses on intelligently processing well-chosen, manually gathered public information.

## 4.6. Automated Knowledge Base Ingestion Pipeline

While initial data curation involves manual gathering of source material (as outlined in Section 4.5), the process of extracting information, transforming it into a graph-compatible format, and loading it into the Amazon Neptune knowledge graph is designed to be an automated, event-driven pipeline. This pipeline leverages several AWS services, including AI capabilities like Amazon Textract and Large Language Models (LLMs) via Amazon Bedrock, to handle diverse source formats and complex extraction logic.

The goal of this pipeline is to:
* Minimize manual data entry into the graph, reducing errors and effort.
* Enable more consistent and repeatable processing of loyalty program information.
* Allow the system to adapt to variations in source document formats through intelligent extraction techniques.
* Provide a scalable mechanism for updating and expanding the knowledge base over time.

The pipeline is orchestrated using AWS Step Functions and involves several key stages detailed below.

### 4.6.1. Overview & Goals of the Pipeline

**High-Level Flow:**
The pipeline is triggered when a `Data Curator` uploads new source material (e.g., HTML files of airline partner pages, PDFs of earning charts, text snippets of rules) to a designated Amazon S3 bucket. From there, a series of automated steps are initiated to process the content:

**[ðŸš§ TODO: Insert Data Ingestion Pipeline Flow Diagram here. See GitHub Issue #6 ðŸš§]**

1.  **Initial Processing & Dispatch:** Basic validation and routing based on file type. PDF/image content is sent for OCR and structure extraction.
2.  **Core Information Extraction:** LLMs are used to understand the content and extract key entities, relationships, and attributes (like earning rates, conditions, effective/expiration dates) based on predefined schemas or targeted prompts.
3.  **Transformation:** The extracted information is transformed into a structured format suitable for the graph model (nodes and edges).
4.  **(Optional) Validation:** Intermediate structured data may be validated for quality and consistency.
5.  **Graph Loading:** The transformed data is loaded into the Amazon Neptune knowledge graph.
6.  **Monitoring & Error Handling:** The entire process is monitored, with mechanisms for logging and handling any errors that occur during processing.

**Primary Goals:**

* **Automation:** To automate the extraction, transformation, and loading (ETL) process for populating the Neptune graph database from diverse, semi-structured source documents.
* **Accuracy:** To employ AI (Textract, LLMs) to achieve a high degree of accuracy in extracting relevant information, including complex rules and specific data points like validity dates.
* **Scalability:** To design a pipeline that can handle an increasing volume of source documents and a growing knowledge graph.
* **Maintainability:** To create a modular pipeline where individual stages can be updated or refined independently.
* **Traceability:** To maintain a link (where possible) between the extracted graph data and its original source document for verification and updates (e.g., using the `WebSourceDocument` node).
* **Efficiency:** To reduce the manual effort required to keep the knowledge graph current and comprehensive.

### 4.6.2. Manual Data Gathering & Staging (Amazon S3)

**Process Overview:**
The entire automated knowledge base ingestion pipeline is predicated on the initial step of manual data gathering by the `Data Curator` persona. This human-in-the-loop approach is chosen to ensure that the source material is relevant, from authoritative public sources (e.g., official airline websites, loyalty program terms and conditions pages, partner agreements), and to navigate complexities like CAPTCHAs or dynamic web content that are beyond the scope of this project's automated tools.

**Responsibilities of the Data Curator:**

* **Identification of Sources:** Identifying web pages, PDF documents, or specific sections of text that contain valuable information about airline loyalty programs, earning rules, redemption policies, partner details, fare class information, validity dates, etc.
* **Data Collection:** Saving the relevant information in a digital format. This may involve:
    * Saving entire HTML web pages.
    * "Printing to PDF" or saving specific online documents as PDFs.
    * Copying and pasting relevant textual sections or tables into simple text files (`.txt`) or snippet HTML files (`.html`).
* **Initial Organization (Optional):** The curator might employ a simple local naming convention or folder structure before upload to help track sources, though the pipeline itself will also add metadata.

**Staging in Amazon S3:**

* **Landing Zone Bucket:** A dedicated Amazon S3 bucket, referred to as the "Raw Source Data Bucket" (e.g., `s3://loyalty-rules-raw-pages/`), serves as the primary landing zone and trigger point for the ingestion pipeline.
* **Upload Mechanism:** The `Data Curator` uploads the gathered files (HTML, PDF, TXT) into this S3 bucket.
    * This could be done via the AWS Management Console, AWS CLI, or a custom-built simple upload interface (though the latter is a future enhancement outside the core pipeline design).
* **Object Key/Prefix Strategy (Conceptual):** To aid organization and potential targeted processing, a prefix strategy within the S3 bucket can be employed. For example:
    * `s3://loyalty-rules-raw-pages/html/[airline_name]/[page_category]/[filename.html]`
    * `s3://loyalty-rules-raw-pages/pdf/[airline_name]/[document_type]/[filename.pdf]`
    * `s3://loyalty-rules-raw-pages/text_snippets/[topic]/[filename.txt]`
      While the pipeline will be designed to handle various inputs, such organization can be beneficial for tracking and potential future re-processing of specific source types.
* **Trigger for Automation:** The creation of new objects (`s3:ObjectCreated:*` events) in this S3 bucket will be the primary trigger that initiates the automated processing pipeline via an S3 event notification (typically to an AWS Lambda function or an AWS Step Functions state machine).

This manual gathering followed by S3 staging effectively decouples the complexities of web interaction and source discovery from the subsequent automated extraction, transformation, and loading processes. It ensures that the automated pipeline works with a known set of inputs provided by a human curator.

### 4.6.3. Pipeline Orchestration (AWS Step Functions)

To manage the multi-step process of ingesting raw source material, extracting information, transforming it, and loading it into the Amazon Neptune knowledge graph, a robust orchestration layer is essential. **AWS Step Functions** will be utilized to define, coordinate, and visualize this complex workflow.

**Rationale for AWS Step Functions:**

* **Workflow Management:** Step Functions allows for the definition of workflows as a series of steps (states), including choices, parallel execution, waits, and error handling. This is ideal for the sequential yet potentially branching logic of the data ingestion pipeline.
* **Serverless Orchestration:** It integrates seamlessly with AWS Lambda (which will execute many of the individual processing tasks), AWS Glue (for ETL jobs), Amazon Bedrock/SageMaker (for LLM calls), and other AWS services.
* **State Management:** Step Functions can manage the state between different steps in the workflow, passing data from one step to the next. This is crucial for tracking the progress of a document through the pipeline.
* **Error Handling & Retries:** Provides built-in mechanisms for error handling (catch/retry logic) for each step, making the pipeline more resilient to transient failures.
* **Visibility & Monitoring:** Offers a visual representation of the workflow execution, making it easier to monitor progress, diagnose issues, and understand the pipeline's state. Execution history is logged in Amazon CloudWatch.
* **Scalability:** Step Functions can scale to handle a large number of concurrent workflow executions.

**Conceptual State Machine Design:**

While the detailed state machine definition (written in the Amazon States Language) is a lower-level design artifact, the conceptual flow orchestrated by Step Functions would be as follows:

1.  **Pipeline Trigger (Initiation):**
    * An S3 `ObjectCreated` event in the "Raw Source Data Bucket" (e.g., `s3://loyalty-rules-raw-pages/`) triggers an initial AWS Lambda function.
    * This Lambda function performs basic validation (e.g., file type, size if necessary) and then initiates a new execution of the Step Functions state machine, passing in details like the S3 object key.

2.  **State: Initial Processing & Dispatch:**
    * A Lambda task state that determines the type of the input file (e.g., HTML, PDF, TXT) from the S3 trigger event.
    * Based on the file type, it will:
        * **For PDFs or image-based documents:**
            * Initiate an asynchronous Amazon Textract job (e.g., `StartDocumentAnalysis` for table and form extraction).
            * The Textract job will be configured to publish its completion status to an Amazon SNS topic.
            * The Step Functions workflow will then pause using the `.waitForTaskToken` integration pattern. A separate AWS Lambda function, subscribed to the SNS topic, will receive the Textract completion notification, retrieve the results, and then send a `SendTaskSuccess` or `SendTaskFailure` call back to Step Functions using the task token to resume the workflow.
        * **For HTML/TXT files:**
            * Perform initial text cleaning or HTML-to-text conversion directly within the Lambda function.
    * **Output:** Path to the S3 location of the extracted text content (from HTML/TXT) or the Textract output JSON (from PDF/images). This S3 path is passed to the next state.

3.  **State: Core Information Extraction (LLM-Powered):**
    * A task state (likely invoking a Lambda function or a Glue ETL job) that takes the processed content.
    * This step involves:
        * Chunking the content if it's too large for a single LLM prompt.
        * Dynamically constructing prompts based on the likely content type or target schema.
        * Invoking an LLM via Amazon Bedrock (e.g., using the `extract_loyalty_info_from_document` conceptual MCP tool logic).
        * Handling LLM responses (which should ideally be structured JSON).
    * This state might involve parallel processing if a document is chunked or if multiple extraction tasks are needed.
    * Output: Structured JSON "facts" or records extracted from the document, stored in S3.

4.  **(Optional) State: Intermediate Validation (Human-in-the-Loop or Automated):**
    * If validation is needed beyond what the LLM extraction provides:
        * **Automated:** A Lambda task state could run programmatic validation rules on the extracted JSON.
        * **Human Review (if confidence is low):** Step Functions can pause a workflow and wait for an external signal (e.g., a human approval via a simple API call or a task token). This is an advanced feature but possible.
        * Output: Validated/corrected JSON facts.

5.  **State: Graph Transformation:**
    * A task state, typically an AWS Glue ETL job, that takes the validated structured JSON facts.
    * Transforms these facts into the CSV format required for Amazon Neptune's bulk loader (nodes and edges).
    * Writes the CSV files to a designated S3 location.

6.  **State: Neptune Bulk Load Initiation:**
    * A Lambda task state that is triggered once the Neptune load files are ready in S3.
    * This Lambda initiates the Neptune bulk load command, pointing to the S3 location of the CSVs.

7.  **State: Monitor Neptune Load & Finalize:**
    * Another Lambda task state (or a loop with waits) that monitors the status of the Neptune bulk load job.
    * Upon completion (success or failure):
        * Logs the final status.
        * Potentially moves the original source file to an "archive" or "processed" S3 prefix.
        * If errors occurred, sends notifications or triggers an error handling sub-workflow.

8.  **Error Handling (Integrated throughout):**
    * Each critical task state will have `Catch` blocks to handle errors gracefully, log them, and potentially trigger retry mechanisms or a specific failure path in the state machine.

This Step Functions workflow provides a clear, manageable, and observable way to automate the complex data journey from raw files to a structured, queryable knowledge graph.

### 4.6.4. Initial Processing & Dispatch (AWS Lambda, Amazon Textract for PDFs)

Once the Step Functions workflow is initiated by an S3 object creation event, the first active stage involves initial processing of the uploaded file and dispatching it for appropriate downstream handling based on its type. This stage is typically implemented as an AWS Lambda function.

**Responsibilities of this Stage:**

* **File Type Identification:** Determine the type of the input file (e.g., HTML, PDF, TXT, image formats like JPEG/PNG if applicable) based on its S3 object key (extension) or potentially its content type metadata.
* **Basic Validation (Optional):** Perform any initial checks, such as maximum file size limits or allowed file types, if defined.
* **Content Extraction/Preparation for PDF/Image Files (Amazon Textract):**
    * If the identified file is a PDF or a supported image format likely containing text or tabular data (e.g., a scanned earning chart), this stage invokes Amazon Textract.
    * The `StartDocumentAnalysis` API (for tables, forms, and text) or `StartDocumentTextDetection` API (for raw text) is used to initiate an asynchronous Textract job.
    * As described in the orchestration section (4.6.3), Textract is configured to send its completion notification (success/failure and S3 location of results) to an SNS topic. The Step Functions workflow will wait for this completion signal via the `.waitForTaskToken` pattern, facilitated by a callback Lambda subscribed to the SNS topic.
    * The output from Textract, typically a JSON structure detailing the detected text, lines, words, tables, and form data, is then staged in a designated S3 prefix (e.g., `s3://loyalty-rules-textract-output/`).
* **Content Preparation for Text-Based Files (HTML, TXT):**
    * If the file is HTML, this Lambda function may perform initial cleaning, such as stripping out script tags, style tags, and other irrelevant HTML boilerplate to extract the main textual content. Libraries like BeautifulSoup (packaged with the Lambda function) can be used for this.
    * If the file is plain text (TXT), it might be used as-is or undergo basic cleaning (e.g., removing excessive whitespace).
    * The processed/cleaned text from these files is then staged in a designated S3 prefix (e.g., `s3://loyalty-rules-processed-text/`).
* **Metadata Propagation:** Key metadata, such as the original S3 object key, detected file type, and the S3 path to the newly processed/staged content (Textract JSON output or cleaned text), is passed on to the next stage in the Step Functions workflow.

**Outcome of this Stage:**

Regardless of the original file type, this stage ensures that the content is in a more standardized format (either structured JSON from Textract or cleaned plain text) and staged in a known S3 location, ready for the "Core Information Extraction" stage (4.6.5) which will involve LLM processing. This dispatch logic ensures that different source formats are appropriately pre-processed before being handed off to the more computationally intensive AI extraction steps.

### 4.6.5. Core Information Extraction (AWS Glue ETL with LLMs/Amazon Bedrock)

Following the initial processing and dispatch stage, where source documents are standardized into cleaned text or structured Textract output, this stage focuses on the intelligent extraction of specific loyalty program entities, rules, relationships, and attributes. This is a critical part of the pipeline, leveraging Large Language Models (LLMs) via Amazon Bedrock within an AWS Glue ETL (Extract, Transform, Load) job.

**Rationale for AWS Glue ETL with LLM Integration:**

* **Scalable Data Processing:** AWS Glue provides a serverless Spark or Python Shell environment capable of processing large volumes of text data efficiently.
* **Complex Transformation Logic:** Glue jobs (written in Python or Scala) can implement sophisticated logic for:
    * Reading processed content from S3 (output of stage 4.6.4).
    * Chunking large documents for LLM processing (to manage context window limits and costs).
    * Dynamically constructing prompts for the LLM based on the document type or content.
    * Interacting with the Amazon Bedrock API to invoke chosen foundation models.
    * Parsing and validating the structured JSON output received from the LLM.
    * Performing any necessary post-processing or enrichment of the extracted data.
* **Integration with AWS Ecosystem:** Glue integrates natively with S3 (for input/output) and can interact with other AWS services like Bedrock.
* **Job Management & Scheduling:** Glue provides job management, monitoring, and scheduling capabilities (though in this event-driven pipeline, jobs are triggered by Step Functions).

**Key Steps within this Stage (Executed by the Glue ETL Job):**

1.  **Input Acquisition:**
    * The Glue job is triggered by the Step Functions orchestrator, receiving metadata (e.g., S3 path to the cleaned text file or Textract JSON output from stage 4.6.4).
    * Reads the relevant content from S3.

2.  **Content Chunking (if necessary):**
    * For very long documents, the content is strategically divided into smaller, manageable chunks.
    * Chunking logic aims to preserve semantic context where possible (e.g., keeping related paragraphs or table sections together) to provide sufficient context for the LLM.

3.  **Dynamic Prompt Engineering & LLM Invocation (Amazon Bedrock):**
    * For each content chunk (or the entire document if small enough), a specific prompt is constructed. This prompt instructs the LLM to extract information according to a predefined target schema (as conceptualized in the `extract_loyalty_info_from_document` MCP tool definition in Section 3.5).
    * The prompt will guide the LLM to identify:
        * Key entities (e.g., airline names, partner airlines, fare classes, program names).
        * Specific attributes (e.g., earning rates, point values, cabin types).
        * Crucial metadata (e.g., rule effective dates, expiration dates, conditions, footnotes).
    * The Glue script makes API calls to Amazon Bedrock, sending the content chunk and the engineered prompt to a chosen foundation model (e.g., a model from Anthropic Claude or AI21 Labs, selected for its instruction-following and JSON output capabilities).
    * The LLM processes the input and returns a response, which is prompted to be in a structured JSON format.

4.  **Response Parsing and Validation:**
    * The Glue job parses the JSON response from the LLM.
    * Basic validation is performed:
        * Does the output conform to the expected JSON schema?
        * Are extracted dates in a valid format?
        * Are critical fields present?
    * Confidence scores provided by the LLM (if available and supported by the model/prompting technique) might be logged.

5.  **Data Aggregation & Structuring:**
    * If the document was chunked, extracted information from multiple chunks is aggregated and consolidated.
    * The extracted "facts" or records are standardized. For example, airline names might be canonicalized using a lookup table or a fuzzy matching routine if necessary.

6.  **Output to Staging Area:**
    * The structured, extracted information (typically a collection of JSON objects representing identified rules, entities, and their attributes) is written to a new, dedicated S3 prefix (e.g., `s3://loyalty-rules-llm-extracted-facts/[airline_name]/[document_id]/extracted_data.jsonl`).
    * Using JSON Lines (`.jsonl`) format (one JSON object per line) is often convenient for downstream processing and for potential querying with Amazon Athena.

**Role of Specialized Data Extraction Agents:**
Conceptually, the LLM, when invoked with specific prompts and target schemas for extracting particular types of information from the documents, acts as a **Specialized Data Extraction Agent** within this Glue ETL job. The Glue job itself provides the environment and surrounding logic for this agent to operate effectively on the input data.

This stage transforms semi-structured or unstructured text into a set of structured JSON records, making the valuable information within the source documents machine-readable and ready for further validation or direct transformation into the graph database format.

### 4.6.6. (Optional) Intermediate Validation (Amazon Athena)

While the Core Information Extraction stage (4.6.5) includes basic validation of the LLM's output (e.g., schema conformance, valid date formats), an optional but recommended intermediate validation stage can be implemented using **Amazon Athena**. This stage allows for more complex data quality checks, ad-hoc querying, and analysis of the structured JSON "facts" extracted by the LLM before they are transformed and loaded into the Amazon Neptune knowledge graph.

This step provides an opportunity to identify systematic errors, inconsistencies, or areas where the LLM extraction might need prompt refinement or further post-processing.

**Rationale for using Amazon Athena for Intermediate Validation:**

* **Serverless Querying:** Athena is a serverless, interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL.
* **Schema-on-Read:** It allows you to define a schema over your JSON data stored in S3 (via an AWS Glue Data Catalog table) and query it immediately.
* **Standard SQL:** Enables powerful and familiar SQL queries for data validation, aggregation, and anomaly detection.
* **Cost-Effective:** You pay only for the queries you run.
* **Early Detection of Issues:** Identifying data quality problems at this intermediate stage is more efficient than discovering them after data has been loaded into the graph, which might require more complex rollback or correction procedures.

**Process for Intermediate Validation:**

1.  **Data Staging:**
    * As described in section 4.6.5, the structured JSON output from the LLM-based extraction (e.g., one JSON object per extracted rule or entity, typically in JSON Lines format - `.jsonl`) is stored in a designated S3 prefix (e.g., `s3://loyalty-rules-llm-extracted-facts/`).

2.  **Schema Definition (AWS Glue Data Catalog):**
    * An AWS Glue Crawler can be run over the S3 location to automatically infer the schema of the JSON objects and create a table in the AWS Glue Data Catalog.
    * Alternatively, the schema for this table can be defined manually (e.g., via a CloudFormation template or Glue console) if the JSON structure output by the LLM is well-defined and consistent. This schema would reflect the fields you expect the LLM to extract (e.g., `airlineName`, `partnerAirline`, `fareClassDetails`, `ruleEffectiveDate`, `ruleExpirationDate`, etc.).

3.  **Querying with Amazon Athena:**
    * Once the Glue Data Catalog table is defined, authorized users (e.g., `Data Curator`, `AI/Data Engineers`) can use the Athena query editor in the AWS Management Console, or connect via JDBC/ODBC, to run SQL queries against the extracted data.
    * **Example Validation Queries:**
        * `SELECT * FROM extracted_loyalty_facts WHERE ruleEffectiveDate > ruleExpirationDate;` (To find illogical date ranges)
        * `SELECT airlineName, COUNT(*) as rule_count FROM extracted_loyalty_facts GROUP BY airlineName ORDER BY rule_count DESC;` (To check distribution of extracted rules per airline)
        * `SELECT * FROM extracted_loyalty_facts WHERE partnerAirline IS NULL AND documentSourceIdentifier LIKE '%partner_page%';` (To find potential missing partner airline extractions from partner pages)
        * `SELECT DISTINCT extractedField FROM extracted_loyalty_facts CROSS JOIN UNNEST(fareClassDetails) AS t(detail) WHERE detail.earningRate IS NULL;` (To find fare class earning details where earning rate is missing)
        * `SELECT documentSourceIdentifier, COUNT(*) FROM extracted_loyalty_facts GROUP BY documentSourceIdentifier HAVING COUNT(*) = 0;` (To identify source documents from which no facts were successfully extracted)

4.  **Action on Validation Results:**
    * **Automated Checks:** Some validation queries can be run programmatically (e.g., via a Lambda function scheduled or triggered by Step Functions after the LLM extraction stage). If critical errors are detected, the pipeline could be paused, or alerts sent.
    * **Manual Review:** Results of ad-hoc queries can inform the `Data Curator` or engineers about issues that might require:
        * Refining the LLM prompts used in stage 4.6.5.
        * Adjusting the parsing logic in the Glue ETL job.
        * Correcting the source material if it's inherently ambiguous.
        * Flagging specific documents for manual review before they proceed to graph transformation.

**Integration into Step Functions Workflow:**

* If automated validation checks are implemented, this stage could be a distinct task state in the Step Functions workflow.
* If critical validation failures occur, Step Functions can route the workflow to an error handling state or a manual review loop (e.g., using `.waitForTaskToken`).

Including this optional validation stage significantly enhances the robustness of the data ingestion pipeline by providing a crucial checkpoint for data quality and consistency before the final graph loading. It allows for iterative improvement of the AI-driven extraction process.

### 4.6.7. Graph Transformation & Loading (AWS Glue ETL, AWS Lambda to Amazon Neptune)

Once the extracted information has been processed and (optionally) validated, the next critical stage is to transform this structured data into a graph format suitable for Amazon Neptune and then load it into the database. This typically involves an AWS Glue ETL job for the transformation and an AWS Lambda function to initiate the loading process.

**1. Graph Transformation (AWS Glue ETL Job):**

* **Purpose:** To convert the structured JSON "facts" (output from stage 4.6.5, potentially refined after stage 4.6.6) into the CSV (Comma Separated Values) format required by Amazon Neptune's bulk loader. Neptune's bulk loader is the most efficient way to ingest large amounts of data.
* **Input:** Reads the validated JSON files from the designated S3 location (e.g., `s3://loyalty-rules-llm-extracted-facts/` or a "validated-facts" S3 prefix).
* **Process (within the Glue ETL Python Shell/Spark script):**
    * **Schema Mapping:** The script iterates through the JSON records and maps the extracted fields to the conceptual graph schema defined in Section 4.3.2 (nodes and edges).
    * **Node Creation Logic:**
        * Identifies unique entities (e.g., Airlines, Loyalty Programs, Airports, Earning Rules, Web Source Documents) from the JSON data.
        * Assigns unique IDs (`~id` property for Neptune) to each node. For entities that might be mentioned multiple times across different source documents (like an airline or an airport), the script needs a strategy for de-duplication and using consistent IDs (e.g., using IATA codes for airports/airlines as part of their node ID, or maintaining a lookup mechanism).
        * Constructs rows for node CSV files, including all defined properties (`~label` for the node type, and other custom properties like `name`, `iataCode`, `effectiveDate`, etc.).
    * **Edge Creation Logic:**
        * Identifies relationships between the nodes based on the extracted data (e.g., an `EarningRule` `APPLIES_TO_FARE_CLASS_ON_AIRLINE` an `Airline` and `FareClass`; an `EarningRule` `CREDITS_TO_PROGRAM` a `LoyaltyProgram`; an `Airline` `PARTNERS_WITH` another `Airline`).
        * Constructs rows for edge CSV files, including unique edge IDs (`~id`), the source node ID (`~from`), the target node ID (`~to`), and the edge label (`~label`), along with any properties of the edge.
    * **Data Type Conversion & Formatting:** Ensures all data types match Neptune's requirements (e.g., dates formatted as ISO 8601 strings, booleans as `true`/`false`, numbers as appropriate).
* **Output:** Generates separate CSV files for nodes and edges, adhering to Neptune's bulk load format specifications. These files are written to a dedicated S3 prefix (e.g., `s3://loyalty-rules-neptune-load-files/[timestamp]/nodes/` and `s3://loyalty-rules-neptune-load-files/[timestamp]/edges/`).
    * Example Node CSV (`nodes_airlines.csv`):
        ```csv
        ~id,~label,airlineId:String,name:String,iataCode:String,allianceName:String
        UA_Airline,Airline,UA,"United Airlines",UA,"Star Alliance"
        LH_Airline,Airline,LH,"Lufthansa",LH,"Star Alliance"
        ```
    * Example Edge CSV (`edges_partners_with.csv`):
        ```csv
        ~id,~from,~to,~label,partnershipType:String
        UA_LH_AlliancePartner,UA_Airline,LH_Airline,PARTNERS_WITH,"Alliance Member"
        ```

**2. Neptune Bulk Load Initiation (AWS Lambda):**

* **Purpose:** To trigger and manage the Amazon Neptune bulk loading process once the formatted CSV files are ready in S3.
* **Trigger:** This Lambda function is typically invoked by the Step Functions orchestrator after the Glue ETL transformation job completes successfully. It could also be triggered by S3 `ObjectCreated` events in the Neptune load files S3 prefix if a more decoupled trigger is desired.
* **Process (within the Lambda function):**
    * **Construct Load Request:** The Lambda function constructs a request to the Neptune Loader endpoint. This request includes:
        * `source`: The S3 path to the directory containing the node and edge CSV files.
        * `format`: `csv`.
        * `iamRoleArn`: The ARN of an IAM role that grants Neptune read access to the S3 bucket containing the load files.
        * `region`: The AWS region of the S3 bucket.
        * `mode`: Often `AUTO` or `RESUME`. `NEW` can be used but requires an empty database or careful planning.
        * `failOnError`: Typically set to `FALSE` to allow the load to continue if some records have issues (errors are logged), or `TRUE` to stop on the first error.
        * `parallelism`: Can be set to optimize load speed (e.g., `MEDIUM`, `HIGH`, `LOW`).
        * `updateSingleCardinalityProperties`: `TRUE` or `FALSE` depending on whether you want to overwrite single cardinality properties if a node/edge with the same ID already exists.
    * **Initiate Load:** The Lambda function makes an HTTP POST request to the Neptune instance's `/loader` endpoint (e.g., `https://your-neptune-endpoint:8182/loader`).
    * **Receive Load ID:** Neptune responds with a `loadId` if the request is accepted. This ID is crucial for monitoring the load status.
    * **Pass Load ID to Step Functions:** The Lambda function returns the `loadId` (and potentially the initial status) to the Step Functions workflow for monitoring.

This two-part process (transformation by Glue, load initiation by Lambda) separates concerns and leverages the most efficient methods for preparing data and ingesting it into Amazon Neptune. The Step Functions workflow ensures these steps are executed in the correct order and that errors can be handled.

### 4.6.8. Pipeline Monitoring & Error Handling

A robust data ingestion pipeline requires comprehensive monitoring to track its execution status, identify bottlenecks, and ensure data quality, as well as effective error handling mechanisms to manage failures gracefully. The AI Loyalty Maximizer Suite's ingestion pipeline will leverage AWS native services for these purposes.

**1. Monitoring with Amazon CloudWatch:**

* **Centralized Logging:**
    * All AWS Lambda functions involved in the pipeline (e.g., initial S3 trigger, Textract callback, Neptune load initiator, any custom processing steps) will send their logs to Amazon CloudWatch Logs. This includes standard output, error messages, and custom application logs detailing processing steps and decisions.
    * AWS Glue ETL jobs also integrate with CloudWatch Logs, providing detailed logs for Spark/Python Shell execution, including driver and executor logs, which are invaluable for debugging transformation logic.
    * AWS Step Functions execution history is logged, and service integration events (e.g., Lambda invocation success/failure) are also sent to CloudWatch Logs.
* **Metrics Collection:**
    * **AWS Lambda Metrics:** CloudWatch automatically collects metrics for Lambda functions, such as invocation counts, duration, error rates, and throttles.
    * **AWS Glue Metrics:** Glue provides metrics for ETL jobs, including job run status, execution time, and data processing metrics (e.g., DPU usage).
    * **AWS Step Functions Metrics:** Metrics for state machine executions, including success/failure rates, execution times, and state transition counts.
    * **Amazon S3 Metrics:** Storage metrics, request metrics, and potentially S3 Event Notification delivery failures.
    * **Amazon Neptune Metrics:** Database health, query latency, CPU/memory utilization, and bulk load specific metrics (e.g., records processed, errors).
    * **Custom Metrics:** Lambda functions or Glue jobs can publish custom metrics to CloudWatch (e.g., number of documents processed, number of entities extracted, average LLM response time) to provide deeper insights into pipeline performance and data quality.
* **Dashboards:** Custom CloudWatch Dashboards will be conceptualized to provide a consolidated view of key pipeline metrics, log summaries, and the overall health of the ingestion process. This allows for quick identification of trends or anomalies.
* **Alarms:** CloudWatch Alarms will be configured based on key metrics or log patterns to provide proactive notifications for:
    * Pipeline execution failures (Step Functions execution failed).
    * High error rates in Lambda functions or Glue jobs.
    * Neptune bulk load failures.
    * Significant delays in processing (e.g., a state in Step Functions running longer than expected).
    * Issues with dependent services (e.g., Bedrock API errors).
    * Alarms can trigger notifications via Amazon SNS to an operations team or `Data Curator`.

**2. Error Handling & Retries (AWS Step Functions & Individual Services):**

* **Step Functions Workflow Logic:**
    * **`Catch` Blocks:** Each critical task state within the Step Functions state machine (e.g., Lambda invocations, Glue job runs) will have `Catch` clauses defined. These clauses can catch specific error types and route the workflow to an error handling path.
    * **`Retry` Blocks:** For transient errors (e.g., temporary network issues, service throttling), task states will be configured with `Retry` blocks, specifying retry intervals, backoff rates, and maximum retry attempts.
    * **Failure States:** The state machine will include specific failure states that can log detailed error information and potentially trigger notifications.
    * **Timeouts:** Each state will have appropriate timeouts configured to prevent indefinite hangs.
* **AWS Lambda Error Handling:**
    * Lambda functions will implement try-except blocks in their code to handle exceptions gracefully, log detailed error information, and return structured error responses that Step Functions can interpret.
    * Dead-Letter Queues (DLQs): Asynchronous Lambda invocations (e.g., from S3 events if not directly feeding Step Functions, or from SNS) can be configured with an SQS DLQ to capture events that fail processing after retries.
* **AWS Glue Job Error Handling:**
    * Glue ETL scripts will include error handling and logging. Glue also provides mechanisms for job retry.
* **Neptune Bulk Load Error Reporting:**
    * The Neptune bulk loader provides detailed error logs in S3 if records fail to load. The Lambda function monitoring the load status (or a subsequent Step Functions state) will be responsible for checking these logs and reporting errors.
    * Strategies for handling load errors might include:
        * Logging the errors and proceeding with successfully loaded data.
        * Halting the pipeline and requiring manual intervention.
        * Triggering a sub-workflow to attempt to correct and reload only the failed records.

**3. Manual Intervention & Dead-Letter Processing (Conceptual):**

* For errors that cannot be automatically resolved, the pipeline should facilitate notification to the `Data Curator` or an operations team.
* A conceptual process for reviewing and reprocessing files that failed critical stages (e.g., due to unexpected source format changes or persistent LLM extraction issues) would be defined. This might involve moving failed source files to a specific "quarantine" S3 prefix for investigation.

By implementing comprehensive monitoring and robust error handling, the Automated Knowledge Base Ingestion Pipeline aims to be a reliable and observable system, minimizing data loss and providing insights into its operational health.

---
*This page is part of the AI Loyalty Maximizer Suite - AWS Reference Architecture. For overall context, please see the [Architecture Overview](./00_ARCHITECTURE_OVERVIEW.md) (if you are not already there) or the main [README.md](../README.md) of this repository.*
